{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import (\n",
    "    AmlCompute, Model, Environment, BuildContext, CodeConfiguration, \n",
    "    ManagedOnlineEndpoint, ManagedOnlineDeployment, OnlineRequestSettings, ProbeSettings\n",
    ")\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "import time, sys, json, os\n",
    "from IPython.display import display, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "ml_client = MLClient.from_config(\n",
    "    credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from hf and register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terminal, run `python [build-hf-env.py`](./job-register-hf-model/build-hf-env.py).\n",
    "- In terminal, run `python [hf-model-job.py`](./job-register-hf-model/hf-model-job.py).\n",
    "- It will use a serverless compute to download model from hf. \n",
    "- Once model is in that machine, it will be registered as model asset in workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registered model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepseek-r1-gguf already exists.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-r1-gguf\"\n",
    "\n",
    "# Make the models query fail-safe\n",
    "models = ml_client.models.list()\n",
    "model = next((m for m in models if m.name == model_name), None)\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"Model {model.name} already exists.\")\n",
    "else:\n",
    "    print(f\"Model {model_name} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the environment asset in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment deepseek-r1-gguf-env-gpu already exists.\n"
     ]
    }
   ],
   "source": [
    "env_name = \"deepseek-r1-gguf-env-gpu\"\n",
    "\n",
    "# Make the environments query fail-safe\n",
    "envs = ml_client.environments.list()\n",
    "env = next((e for e in envs if e.name == env_name), None)\n",
    "\n",
    "# Flag to indicate if the environment is modified. \n",
    "# Set it manually:: If True, old env version is used. If False, new env version is created.\n",
    "is_env_earlierone = True\n",
    "\n",
    "if env is not None and is_env_earlierone:\n",
    "    print(f\"Environment {env_name} already exists.\")\n",
    "else:\n",
    "    # Create a new environment based on the curated one\n",
    "    custom_env = Environment(\n",
    "        name=env_name,\n",
    "        description=\"Custom environment with additional dependencies\",\n",
    "        build=BuildContext(path=\"../env/gpu\")\n",
    "    )\n",
    "\n",
    "    # Register the custom environment\n",
    "    ml_client.environments.create_or_update(custom_env)\n",
    "\n",
    "    print(\"Environment is registered and be built now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create managed online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "online_endpoint_name = \"deepseekr1-gguf-ep-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for deepseek-r1 gguf\",\n",
    "    auth_mode=\"key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint create async call\n",
    "ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create managed online deployment for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"deploy01\"\n",
    "model = f\"{model_name}@latest\"\n",
    "env = f\"{env_name}@latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../onlinescoring\",\n",
    "        scoring_script=\"score.py\",\n",
    "    ),\n",
    "    instance_type=\"Standard_NC48ads_A100_v4\",\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=120000,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        initial_delay=600\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint deepseekr1-gguf-ep-gpu exists\n",
      "\u001b[32mUploading onlinescoring (0.02 MBs): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21006/21006 [00:00<00:00, 248778.21it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "# managed endpoint deplyment create async call\n",
    "ml_client.online_deployments.begin_create_or_update(deployment=demo_deployment).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update traffic to the deployment for 100%\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test endpoint with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\": \"<think>\\nOkay, the user is asking for the capital of India. I know that's a common question, so I need to make sure my answer is accurate and clear. Let me think... The capital city of India is New Delhi. Wait, sometimes people confuse it with Delhi. Is there a difference between Delhi and New Delhi? Yes, New Delhi is actually a part of the larger Delhi metropolitan area, which serves as the capital. So I should specify that to avoid confusion. Also, maybe add a bit about its significance, like being a political and administrative hub. That would make the answer more informative. Let me double-check: India's capital is indeed New Delhi. It was established during British rule and became the capital in 1911. Post-independence, it remained the capital. So I can mention that to give some historical context. Make sure not to get mixed up with other cities like Mumbai or Chennai. No, those are different. Okay, so putting it all together: The capital is New Delhi, part of the Delhi area, established as capital in 1911, and important for governance. That should cover their question thoroughly without overcomplicating things.\\n</think>\\n\\nThe capital of India is **New Delhi**, which is a part of the larger Delhi metropolitan area. It serves as the political and administrative heart of the country. New Delhi was officially designated as India's capital in 1911 during British rule, and it has remained the capital since India gained independence in 1947. The city is home to major government institutions, including the Parliament of India and the Rashtrapati Bhavan (the President's residence).\"}\n"
     ]
    }
   ],
   "source": [
    "# score the request json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"../payload/request3.json\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"output\": \"<think>\\\\nOkay, the user is asking for the capital of India. I '\n",
      " \"know that's a common question, so I need to make sure my answer is accurate \"\n",
      " 'and clear. Let me think... The capital city of India is New Delhi. Wait, '\n",
      " 'sometimes people confuse it with Delhi. Is there a difference between Delhi '\n",
      " 'and New Delhi? Yes, New Delhi is actually a part of the larger Delhi '\n",
      " 'metropolitan area, which serves as the capital. So I should specify that to '\n",
      " 'avoid confusion. Also, maybe add a bit about its significance, like being a '\n",
      " 'political and administrative hub. That would make the answer more '\n",
      " \"informative. Let me double-check: India's capital is indeed New Delhi. It \"\n",
      " 'was established during British rule and became the capital in 1911. '\n",
      " 'Post-independence, it remained the capital. So I can mention that to give '\n",
      " 'some historical context. Make sure not to get mixed up with other cities '\n",
      " 'like Mumbai or Chennai. No, those are different. Okay, so putting it all '\n",
      " 'together: The capital is New Delhi, part of the Delhi area, established as '\n",
      " 'capital in 1911, and important for governance. That should cover their '\n",
      " 'question thoroughly without overcomplicating things.\\\\n</think>\\\\n\\\\nThe '\n",
      " 'capital of India is **New Delhi**, which is a part of the larger Delhi '\n",
      " 'metropolitan area. It serves as the political and administrative heart of '\n",
      " \"the country. New Delhi was officially designated as India's capital in 1911 \"\n",
      " 'during British rule, and it has remained the capital since India gained '\n",
      " 'independence in 1947. The city is home to major government institutions, '\n",
      " 'including the Parliament of India and the Rashtrapati Bhavan (the '\n",
      " 'President\\'s residence).\"}')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Assuming response is a dict\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\": \"<think>\\nOkay, so I'm trying to understand the concept of Mixture of Experts (MoE) in Large Language Models. The user mentioned first principles thinking, which means breaking it down into fundamental ideas and building up from there. Let me start by recalling what I know about machine learning models.\\n\\nI remember that traditional neural networks have layers where each neuron is connected to all others in the next layer\\u2014like a fully connected layer. This can get really big as the model size increases, leading to high computational costs. But with MoE, maybe they found a way to make this more efficient?\\n\\nThe term \\\"Mixture of Experts\\\" sounds like it combines multiple specialized models (experts) and uses them together somehow. So instead of one big network, you have several smaller ones? Maybe each expert handles different parts of the input data. But how does that work exactly in the context of language models?\\n\\nI recall something about routing mechanisms. The model needs to decide which experts to consult for a given input. There's probably a gate that directs the input to relevant experts. So, when processing a word or token, only certain experts are activated, reducing computation since not all neurons are used every time.\\n\\nWait, but how does training work here? If each expert is specialized, they must be trained on specific aspects of data. But during backpropagation, do you train both the gate and the experts simultaneously? Also, how do you prevent one expert from dominating others or some being underutilized?\\n\\nOh, there's a balance between having more specialists (experts) but ensuring that each gets enough attention. Maybe the loss function includes something to encourage diversity in expert usage. Like a penalty if only one expert is used all the time.\\n\\nThe idea of sparsity comes into play here too. Since each input doesn't use all experts, activation is sparse which saves on computation. But how does this compare to models like Transformers? In Transformer MoE layers, they replace dense feed-forward layers with sparse ones composed of multiple experts.\\n\\nSo for a given token, the model selects top-k experts (maybe two), aggregates their outputs. This way, even though there are many experts, only a few contribute per input, keeping computation manageable. But how is this implemented technically? The routing matrix must compute weights based on the inputs, then multiply with expert outputs.\\n\\nI'm still confused about why MoE works better in some cases. Is it because different parts of the data require different processing? For example, handling various linguistic structures or topics, each handled by a specialized network?\\n\\nAlso, how does this affect model capacity versus computational cost? More experts mean more parameters but lower activation since only part is used per input. So total parameters are higher, but actual computation during inference scales with number of active experts.\\n\\nI need to structure these thoughts into fundamental concepts and build up step-by-step.\\n</think>\\n\\nCertainly! Let's break down the Mixture of Experts (MoE) concept in Large Language Models (LLMs) using first principles thinking:\\n\\n---\\n\\n### **1. Fundamental Concepts**\\n\\n#### **a. Neural Networks Basics**\\n- A neural network consists of layers where neurons are interconnected, processing inputs through weighted connections and non-linear activations.\\nDeeper networks increase capacity but also computational cost.\\n\\n#### **b. Problem with Scale**\\nDensely connected layers (all neurons fully connected) become computationally expensive as models grow. This leads to inefficiency in both training and inference.\\n\\n---\\n\\n### **2. Core Idea of MoE**\\n\\n- Instead of a single monolithic network, distribute computation across multiple specialized subnetworks (\\\"experts\\\").\\n  - Each expert focuses on specific input patterns.\\n  - A routing mechanism dynamically selects which experts to consult for each input.\\n\\n**Key Insight:** Leverage specialization and sparsity to maintain model capacity while reducing computational overhead.\\n\\n---\\n\\n### **3. Building Blocks**\\n\\n#### **a. Experts**\\n- Multiple parallel subnetworks, each with its own parameters.\\n- Each expert specializes in certain aspects of the data (e.g., linguistic structures or topics).\\n\\n**Example:** 100 experts, each a small feed-forward network.\\n\\n#### **b. Gating Mechanism**\\nDynamically routes inputs to relevant experts:\\n1. Compute routing weights via a trainable gating function:  \\n   \\\\( \\\\mathbf{g} = \\\\text{Softmax}(W_g \\\\cdot \\\\mathbf{x}) \\\\) \\n2. Select top-k experts (e.g., k=2) based on highest weights.\\n\\n**Result:** Each input activates only a subset of experts, introducing sparsity and reducing computation.\\n\\n#### **c. Aggregation**\\nCombine outputs from selected experts:\\n\\\\( \\\\text{Output} = \\\\sum_{i \\\\in \\\\text{top-k}} g_i \\\\cdot E_i(\\\\mathbf{x}) \\\\)  \\nWhere \\\\( E_i \\\\) is the i-th expert's output.\\n\\n---\\n\\n### **4. Training Dynamics**\\n\\n- **Joint Optimization:** Both experts and gate are trained together via backpropagation.\\n  - Gate learns to route inputs effectively.\\n  - Experts learn specialized functions for different input patterns.\\n\\n- **Balancing Experts:** Use auxiliary loss terms or constraints to prevent dominance by a few experts (e.g., encouraging uniform routing weights).\\n\\n---\\n\\n### **5. Efficiency vs. Capacity**\\n\\n- **Capacity:** Total parameters increase with more experts, enabling high expressivity.\\n- **Efficiency:** During inference, only k experts are active per input, keeping computational cost proportional to \\\\(k \\\\times \\\\) expert size.\\n\\n**Tradeoff:** Higher parameter count but lower activation cost compared to dense models.\\n\\n---\\n\\n### **6. Why It Works**\\n\\n- **Specialization:** Experts capture diverse patterns in complex data (e.g., language nuances).\\n- **Sparsity:** Reduces redundant computation by activating relevant subsets.\\n- **Scalability:** Scales model capacity without proportionally increasing compute.\\n\\n---\\n\\n### **7. Challenges & Solutions**\\n\\n#### **a. Expert Imbalance**\\n**Problem:** Some experts may be underutilized.  \\n**Solution:** Regularization to encourage uniform routing or load balancing (e.g., Google's \\\"GShard\\\" for distributed training).\\n\\n#### **b. Routing Complexity**\\n**Problem:** Na\\u00efve routing scales with number of experts.  \\n**Solution:** Efficient top-k selection and sparse implementations.\\n\\n---\\n\\n### **8. Summary**\\n\\nMoE replaces dense layers with sparsely activated expert groups, allowing LLMs to scale efficiently by:\\n1. Distributing computation across specialists.\\n2. Maintaining high capacity via increased parameters.\\n3. Limiting compute through dynamic, sparse routing.\\n\\nThis approach enables models like GPT-4 and others to achieve higher performance without proportional increases in computational demand during inference.\"}\n"
     ]
    }
   ],
   "source": [
    "# score the request json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"../payload/request4_reasoning.json\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"output\": \"<think>\\\\nOkay, so I\\'m trying to understand the concept of '\n",
      " 'Mixture of Experts (MoE) in Large Language Models. The user mentioned first '\n",
      " 'principles thinking, which means breaking it down into fundamental ideas and '\n",
      " 'building up from there. Let me start by recalling what I know about machine '\n",
      " 'learning models.\\\\n\\\\nI remember that traditional neural networks have '\n",
      " 'layers where each neuron is connected to all others in the next '\n",
      " 'layer\\\\u2014like a fully connected layer. This can get really big as the '\n",
      " 'model size increases, leading to high computational costs. But with MoE, '\n",
      " 'maybe they found a way to make this more efficient?\\\\n\\\\nThe term \\\\\"Mixture '\n",
      " 'of Experts\\\\\" sounds like it combines multiple specialized models (experts) '\n",
      " 'and uses them together somehow. So instead of one big network, you have '\n",
      " 'several smaller ones? Maybe each expert handles different parts of the input '\n",
      " 'data. But how does that work exactly in the context of language '\n",
      " 'models?\\\\n\\\\nI recall something about routing mechanisms. The model needs to '\n",
      " \"decide which experts to consult for a given input. There's probably a gate \"\n",
      " 'that directs the input to relevant experts. So, when processing a word or '\n",
      " 'token, only certain experts are activated, reducing computation since not '\n",
      " 'all neurons are used every time.\\\\n\\\\nWait, but how does training work here? '\n",
      " 'If each expert is specialized, they must be trained on specific aspects of '\n",
      " 'data. But during backpropagation, do you train both the gate and the experts '\n",
      " 'simultaneously? Also, how do you prevent one expert from dominating others '\n",
      " \"or some being underutilized?\\\\n\\\\nOh, there's a balance between having more \"\n",
      " 'specialists (experts) but ensuring that each gets enough attention. Maybe '\n",
      " 'the loss function includes something to encourage diversity in expert usage. '\n",
      " 'Like a penalty if only one expert is used all the time.\\\\n\\\\nThe idea of '\n",
      " \"sparsity comes into play here too. Since each input doesn't use all experts, \"\n",
      " 'activation is sparse which saves on computation. But how does this compare '\n",
      " 'to models like Transformers? In Transformer MoE layers, they replace dense '\n",
      " 'feed-forward layers with sparse ones composed of multiple experts.\\\\n\\\\nSo '\n",
      " 'for a given token, the model selects top-k experts (maybe two), aggregates '\n",
      " 'their outputs. This way, even though there are many experts, only a few '\n",
      " 'contribute per input, keeping computation manageable. But how is this '\n",
      " 'implemented technically? The routing matrix must compute weights based on '\n",
      " \"the inputs, then multiply with expert outputs.\\\\n\\\\nI'm still confused about \"\n",
      " 'why MoE works better in some cases. Is it because different parts of the '\n",
      " 'data require different processing? For example, handling various linguistic '\n",
      " 'structures or topics, each handled by a specialized network?\\\\n\\\\nAlso, how '\n",
      " 'does this affect model capacity versus computational cost? More experts mean '\n",
      " 'more parameters but lower activation since only part is used per input. So '\n",
      " 'total parameters are higher, but actual computation during inference scales '\n",
      " 'with number of active experts.\\\\n\\\\nI need to structure these thoughts into '\n",
      " 'fundamental concepts and build up step-by-step.\\\\n</think>\\\\n\\\\nCertainly! '\n",
      " \"Let's break down the Mixture of Experts (MoE) concept in Large Language \"\n",
      " 'Models (LLMs) using first principles thinking:\\\\n\\\\n---\\\\n\\\\n### **1. '\n",
      " 'Fundamental Concepts**\\\\n\\\\n#### **a. Neural Networks Basics**\\\\n- A neural '\n",
      " 'network consists of layers where neurons are interconnected, processing '\n",
      " 'inputs through weighted connections and non-linear activations.\\\\nDeeper '\n",
      " 'networks increase capacity but also computational cost.\\\\n\\\\n#### **b. '\n",
      " 'Problem with Scale**\\\\nDensely connected layers (all neurons fully '\n",
      " 'connected) become computationally expensive as models grow. This leads to '\n",
      " 'inefficiency in both training and inference.\\\\n\\\\n---\\\\n\\\\n### **2. Core '\n",
      " 'Idea of MoE**\\\\n\\\\n- Instead of a single monolithic network, distribute '\n",
      " 'computation across multiple specialized subnetworks (\\\\\"experts\\\\\").\\\\n  - '\n",
      " 'Each expert focuses on specific input patterns.\\\\n  - A routing mechanism '\n",
      " 'dynamically selects which experts to consult for each input.\\\\n\\\\n**Key '\n",
      " 'Insight:** Leverage specialization and sparsity to maintain model capacity '\n",
      " 'while reducing computational overhead.\\\\n\\\\n---\\\\n\\\\n### **3. Building '\n",
      " 'Blocks**\\\\n\\\\n#### **a. Experts**\\\\n- Multiple parallel subnetworks, each '\n",
      " 'with its own parameters.\\\\n- Each expert specializes in certain aspects of '\n",
      " 'the data (e.g., linguistic structures or topics).\\\\n\\\\n**Example:** 100 '\n",
      " 'experts, each a small feed-forward network.\\\\n\\\\n#### **b. Gating '\n",
      " 'Mechanism**\\\\nDynamically routes inputs to relevant experts:\\\\n1. Compute '\n",
      " 'routing weights via a trainable gating function:  \\\\n   \\\\\\\\( \\\\\\\\mathbf{g} '\n",
      " '= \\\\\\\\text{Softmax}(W_g \\\\\\\\cdot \\\\\\\\mathbf{x}) \\\\\\\\) \\\\n2. Select top-k '\n",
      " 'experts (e.g., k=2) based on highest weights.\\\\n\\\\n**Result:** Each input '\n",
      " 'activates only a subset of experts, introducing sparsity and reducing '\n",
      " 'computation.\\\\n\\\\n#### **c. Aggregation**\\\\nCombine outputs from selected '\n",
      " 'experts:\\\\n\\\\\\\\( \\\\\\\\text{Output} = \\\\\\\\sum_{i \\\\\\\\in \\\\\\\\text{top-k}} g_i '\n",
      " '\\\\\\\\cdot E_i(\\\\\\\\mathbf{x}) \\\\\\\\)  \\\\nWhere \\\\\\\\( E_i \\\\\\\\) is the i-th '\n",
      " \"expert's output.\\\\n\\\\n---\\\\n\\\\n### **4. Training Dynamics**\\\\n\\\\n- **Joint \"\n",
      " 'Optimization:** Both experts and gate are trained together via '\n",
      " 'backpropagation.\\\\n  - Gate learns to route inputs effectively.\\\\n  - '\n",
      " 'Experts learn specialized functions for different input patterns.\\\\n\\\\n- '\n",
      " '**Balancing Experts:** Use auxiliary loss terms or constraints to prevent '\n",
      " 'dominance by a few experts (e.g., encouraging uniform routing '\n",
      " 'weights).\\\\n\\\\n---\\\\n\\\\n### **5. Efficiency vs. Capacity**\\\\n\\\\n- '\n",
      " '**Capacity:** Total parameters increase with more experts, enabling high '\n",
      " 'expressivity.\\\\n- **Efficiency:** During inference, only k experts are '\n",
      " 'active per input, keeping computational cost proportional to \\\\\\\\(k '\n",
      " '\\\\\\\\times \\\\\\\\) expert size.\\\\n\\\\n**Tradeoff:** Higher parameter count but '\n",
      " 'lower activation cost compared to dense models.\\\\n\\\\n---\\\\n\\\\n### **6. Why '\n",
      " 'It Works**\\\\n\\\\n- **Specialization:** Experts capture diverse patterns in '\n",
      " 'complex data (e.g., language nuances).\\\\n- **Sparsity:** Reduces redundant '\n",
      " 'computation by activating relevant subsets.\\\\n- **Scalability:** Scales '\n",
      " 'model capacity without proportionally increasing compute.\\\\n\\\\n---\\\\n\\\\n### '\n",
      " '**7. Challenges & Solutions**\\\\n\\\\n#### **a. Expert '\n",
      " 'Imbalance**\\\\n**Problem:** Some experts may be underutilized.  '\n",
      " '\\\\n**Solution:** Regularization to encourage uniform routing or load '\n",
      " 'balancing (e.g., Google\\'s \\\\\"GShard\\\\\" for distributed training).\\\\n\\\\n#### '\n",
      " '**b. Routing Complexity**\\\\n**Problem:** Na\\\\u00efve routing scales with '\n",
      " 'number of experts.  \\\\n**Solution:** Efficient top-k selection and sparse '\n",
      " 'implementations.\\\\n\\\\n---\\\\n\\\\n### **8. Summary**\\\\n\\\\nMoE replaces dense '\n",
      " 'layers with sparsely activated expert groups, allowing LLMs to scale '\n",
      " 'efficiently by:\\\\n1. Distributing computation across specialists.\\\\n2. '\n",
      " 'Maintaining high capacity via increased parameters.\\\\n3. Limiting compute '\n",
      " 'through dynamic, sparse routing.\\\\n\\\\nThis approach enables models like '\n",
      " 'GPT-4 and others to achieve higher performance without proportional '\n",
      " 'increases in computational demand during inference.\"}')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
