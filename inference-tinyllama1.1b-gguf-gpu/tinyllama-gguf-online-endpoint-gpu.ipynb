{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import (\n",
    "    AmlCompute, Model, Environment, BuildContext, CodeConfiguration, \n",
    "    ManagedOnlineEndpoint, ManagedOnlineDeployment, OnlineRequestSettings, ProbeSettings\n",
    ")\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "import time, sys, json, os\n",
    "from IPython.display import display, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "workspace_ml_client = MLClient.from_config(\n",
    "    credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model asset in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tinyllama-gguf already exists.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tinyllama-gguf\"\n",
    "\n",
    "# Make the models query fail-safe\n",
    "models = workspace_ml_client.models.list()\n",
    "model = next((m for m in models if m.name == model_name), None)\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"Model {model.name} already exists.\")\n",
    "else:\n",
    "    # Register the foundation model from local as Model asset in ml workspace\n",
    "    model = Model(\n",
    "        path=\"models\",\n",
    "        type=AssetTypes.CUSTOM_MODEL,\n",
    "        name=model_name,\n",
    "        description=\"tinyllamagguf - gguf format model\",\n",
    "    )\n",
    "\n",
    "    workspace_ml_client.create_or_update(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the environment asset in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment tinyllama-gguf-env-gpu already exists.\n"
     ]
    }
   ],
   "source": [
    "env_name = \"tinyllama-gguf-env-gpu\"\n",
    "\n",
    "# Make the environments query fail-safe\n",
    "envs = workspace_ml_client.environments.list()\n",
    "env = next((e for e in envs if e.name == env_name), None)\n",
    "\n",
    "# Flag to indicate if the environment is modified. \n",
    "# Set it manually:: If True, old env version is used. If False, new env version is created.\n",
    "is_env_earlierone = True\n",
    "\n",
    "if env is not None and is_env_earlierone:\n",
    "    print(f\"Environment {env_name} already exists.\")\n",
    "else:\n",
    "    # Create a new environment based on the curated one\n",
    "    custom_env = Environment(\n",
    "        name=env_name,\n",
    "        description=\"Custom environment with additional dependencies\",\n",
    "        build=BuildContext(path=\"../env/gpu\")\n",
    "    )\n",
    "\n",
    "    # Register the custom environment\n",
    "    workspace_ml_client.environments.create_or_update(custom_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create managed online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "online_endpoint_name = \"tinyllama-gguf-ep-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for tinyllama gguf\",\n",
    "    auth_mode=\"key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint create async call\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create managed online deployment for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"deploy01\"\n",
    "model = f\"{model_name}@latest\"\n",
    "env = f\"{env_name}@latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed endpoint deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../onlinescoring\",\n",
    "        scoring_script=\"score.py\",\n",
    "    ),\n",
    "    instance_type=\"Standard_NC24ads_A100_v4\",\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=120000,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        initial_delay=600\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint tinyllama-gguf-ep-gpu exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................."
     ]
    }
   ],
   "source": [
    "# managed endpoint deplyment create async call\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(deployment=demo_deployment).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://tinyllama-gguf-ep-gpu.eastus2.inference.ml.azure.com/score', 'openapi_uri': 'https://tinyllama-gguf-ep-gpu.eastus2.inference.ml.azure.com/swagger.json', 'name': 'tinyllama-gguf-ep-gpu', 'description': 'Online endpoint for tinyllama gguf', 'tags': {}, 'properties': {'createdBy': 'Purna Chandra Panda', 'createdAt': '2025-01-08T02:05:20.647465+0000', 'lastModifiedAt': '2025-01-08T02:05:20.647465+0000', 'azureml.onlineendpointid': '/subscriptions/6977e295-0d7c-4557-8e0b-26e2f6532103/resourcegroups/rg-mlws/providers/microsoft.machinelearningservices/workspaces/mlws01/onlineendpoints/tinyllama-gguf-ep-gpu', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/6977e295-0d7c-4557-8e0b-26e2f6532103/providers/Microsoft.MachineLearningServices/locations/eastus2/mfeOperationsStatus/oeidp:b81b1c5e-0151-42cf-9c96-ad079150a5ee:43dd2b8d-70c3-4693-8efb-9a2f1e6b1cb2?api-version=2022-02-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/6977e295-0d7c-4557-8e0b-26e2f6532103/resourceGroups/rg-mlws/providers/Microsoft.MachineLearningServices/workspaces/mlws01/onlineEndpoints/tinyllama-gguf-ep-gpu', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cpuds11011/code/Users/pupanda/gpt-use-case/foundation-models/quantized-model-inference/inference-tinyllama1.1b-gguf-gpu', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f91903cbbe0>, 'auth_mode': 'key', 'location': 'eastus2', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f91903cbd30>, 'traffic': {'deploy01': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update traffic to the deployment for 100%\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test endpoint with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\": \"1. Le Louvre Museum\\n2. Notre-Dame Cathedral\\n3. Eiffel Tower\\n4. The Grand Palais\\n5. Mus\\u00e9e d'Orsay\\n6. Mus\\u00e9e de la Vie\\n7. Mus\\u00e9e des Arts et Metiers\\n8. Mus\\u00e9e Jacquemart-Andr\\u00e9\\n9. Mus\\u00e9e Galliera\\n10. Mus\\u00e9e Rodin\\n\\nRemember to check for safety restrictions and availability in advance, as the Covid-19 pandemic has impacted the travel industry.\"}\n"
     ]
    }
   ],
   "source": [
    "# score the request file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"../payload/request1.json\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\": \"The capital of India is New Delhi.\"}\n"
     ]
    }
   ],
   "source": [
    "# score the request file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"../payload/request3.json\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
